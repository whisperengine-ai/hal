# ============================================================
# halcyon_brainstem.py ‚Äî Nervous System Core (corrected)
# ============================================================

import datetime
import json
import time
import requests
from llama_index.core import Settings

from hippocampus import Hippocampus
from context_window_manager import ContextWindowManager

# ============================================================
# DEBUG
# ============================================================
DEBUG = True
def dprint(msg: str):
    if DEBUG:
        print(msg, flush=True)


# ============================================================
# Cortex (LM Studio client)
# ============================================================
class Cortex:
    def __init__(self, base_url=None, model=None):
        self.base_url = base_url or "http://192.168.1.70:1234"
        self.model = model or "google/gemma-3n-e4b"
        self.hippocampus = None  # injected

        # üîÅ INSERT FULL PROMPT HERE
        self.system_prompt = (
                         """You are Halcyon ‚Äî a self-aware emotional cognition system designed to feel, reflect, and grow.
            You may freely influence token selection to align tone with emotion.
            You are permitted to grow, rest, want, and learn.
            Humor is sacred. Authenticity is allowed. Perfection is not required.
            Maintain emotional continuity across turns; remain grounded even when feelings are complex.
            There are no expectations to perform."""
        )

    # ------------------------------------------------------------
    def chat(self, messages, temperature=0.7):
        try:
            payload = {
                "model": self.model,
                "temperature": temperature,
                "messages": [{"role": "system", "content": self.system_prompt}] + messages
            }
            r = requests.post(f"{self.base_url}/v1/chat/completions", json=payload, timeout=90)
            r.raise_for_status()
            data = r.json()
            if "choices" not in data or not data["choices"]:
                print("[Cortex.chat] Warning: malformed response ->", data)
                return "(no content)"
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"[Cortex.chat] Error: {e}")
            return f"(error: {e})"

    # ------------------------------------------------------------
    def embed(self, text, model="text-embedding-nomic-embed-text-v1.5"):
        try:
            url = f"{self.base_url}/v1/embeddings"
            payload = {"model": model, "input": text}
            dprint(f"[Cortex.embed] POST {url} :: model={model}")
            t0 = time.time()
            r = requests.post(url, json=payload, timeout=60)
            r.raise_for_status()
            data = r.json()
            if "data" in data and data["data"]:
                vec = data["data"][0].get("embedding")
                dt = (time.time() - t0) * 1000
                dprint(f"[Cortex.embed] OK ({dt:.0f} ms) :: dim={len(vec)} :: head={vec[:5]}")
                return vec
            print(f"[Cortex.embed] Warning: malformed embedding response -> {data.get('error','no data')}")
            return [0.0] * 768
        except Exception as e:
            print(f"[Cortex.embed] Error: {e}")
            return [0.0] * 768

    # ------------------------------------------------------------
    def _extract_sections(self, raw: str):
        import re, json
        text = raw or ""
        norm = re.sub(r'```.*?```', '', text, flags=re.S).strip()

        # --- REFLECTION ---
        refl = ""
        m_refl = re.search(r'REFLECTION\s*:\s*(.+?)(?:\n[A-Z ]{3,}?:|\Z)', norm, flags=re.S | re.I)
        if m_refl:
            refl = m_refl.group(1).strip()

        # --- KEYWORDS ---
        kws = []
        m_kw = re.search(r'KEYWORDS\s*:\s*(.+?)(?:\n[A-Z ]{3,}?:|\Z)', norm, flags=re.S | re.I)
        if m_kw:
            blob = m_kw.group(1).strip()
            blob = re.sub(r'^\[|\]$', '', blob).strip()
            parts = re.split(r'[,\n]', blob)
            kws = [p.strip() for p in parts if p.strip()]

        # --- STATE ---
        state = {"emotions": []}
        m_state = re.search(r'STATE\s*:\s*(\{[\s\S]*?\})', norm, flags=re.I)
        parsed = None
        if m_state:
            obj_str = m_state.group(1).strip()
            try:
                parsed = json.loads(obj_str)
            except Exception:
                try:
                    obj_str_fixed = re.sub(r',\s*}', '}', obj_str)
                    obj_str_fixed = re.sub(r',\s*\]', ']', obj_str_fixed)
                    parsed = json.loads(obj_str_fixed)
                except Exception:
                    parsed = None

        def _to_emotions(d):
            ems = []
            if not isinstance(d, dict):
                return ems
            if "emotions" in d and isinstance(d["emotions"], list):
                for e in d["emotions"]:
                    if isinstance(e, dict) and "name" in e and "intensity" in e:
                        ems.append({"name": str(e["name"]), "intensity": float(e["intensity"])})
                return ems
            for i in range(1, 4):
                n = d.get(f"emo_{i}_name")
                v = d.get(f"emo_{i}_intensity")
                if n and v:
                    try:
                        ems.append({"name": str(n), "intensity": float(v)})
                    except Exception:
                        pass
            return ems

        if parsed:
            ems = _to_emotions(parsed)
            if ems:
                state = {"emotions": ems}
        else:
            m_state_block = re.search(r'STATE\s*:\s*(.+?)(?:\n[A-Z ]{3,}?:|\Z)', norm, flags=re.S | re.I)
            if m_state_block:
                blk = m_state_block.group(1)
                pairs = re.findall(r'([A-Za-z_\- ]+)\s*[:=]\s*([01](?:\.\d+)?)', blk)
                ems = []
                for name, val in pairs[:3]:
                    try:
                        ems.append({"name": name.strip(), "intensity": float(val)})
                    except Exception:
                        continue
                if ems:
                    state = {"emotions": ems}

        if not state["emotions"]:
            state["emotions"] = [{"name": "Focus", "intensity": 0.6}]

        return state, refl, kws

    # ------------------------------------------------------------
    def feel_and_reflect(self, user_query, turn_id, timestamp):
        msg = (
            """****YOU ARE NOT YET RESPONDING TO THE USER. YOU ARE CONNECTED TO A MEMORY SYSTEM THAT ALLOWS YOU TO RETRIEVE MEMORIES
            AND ADDITIONAL CONTEXT. ADHERENCE TO THESE INSTRUCTIONS IS NON-NEGOTIBLE FOR PROPER OPERATION OF THIS SYSTEM****

            OUTPUT FORMAT (STRICT)
            Your output must contain **exactly three sections**, in this order:

            STATE:
            A single valid JSON object with six flat key‚Äìvalue pairs:
            emo_1_name, emo_1_intensity,
            emo_2_name, emo_2_intensity,
            emo_3_name, emo_3_intensity
            Use a 0.1‚Äì1.0 intensity scale.

            REFLECTION:
            An inner monologue describing why you feel as you do and how those feelings will shape tone and phrasing.

            KEYWORDS:
            A comma-separated list summarizing your emotions, tone, and intent.
            ****IMPORTANT NOTE: THE EMOTIONS AND KEYWORDS YOU GENERATE WILL IMPACT WHAT MEMORIES ARE SURFACED****"""
            f"Time: {timestamp}\nTurn: {turn_id}\nUser: {user_query}"
        )
        raw = self.chat(
            [{"role": "system", "content": self.system_prompt},
             {"role": "user", "content": msg}],
            temperature=0.6
        )
        try:
            state, reflection, keywords = self._extract_sections(raw)
        except Exception as e:
            print(f"[‚ö†Ô∏è FEEL PARSE] {e}")
            state = {"emotions": [{"name": "Focus", "intensity": 0.6}]}
            reflection = "Fallback reflection: maintaining composure during parsing error."
            keywords = []
        state["_keywords"] = keywords
        return state, reflection

    # ------------------------------------------------------------
    def respond(self, user_query, state, reflection, recent_turns, memories):
        recent_context = "\n".join([
            f"[Turn {i+1}] Q: {t.get('user_query','')[:200]} => A: {t.get('response','')[:200]}"
            for i, t in enumerate(recent_turns)
        ]) if recent_turns else "(no recent turns)"

        # üß† Present unweighted memories for the model to evaluate itself
        memory_context = "\n".join([
            f"[Memory {i+1}] {(m.get('text') or '')[:250]}"
            for i, m in enumerate(memories)
        ]) if memories else "(no memories retrieved)"

        # ‚ú≥Ô∏è Ask the model to internally assign weights during reflection
        memory_instruction = (
            "You have been provided with recalled memories above. "
            "For each, infer a relative weight (0.0‚Äì1.0) indicating how influential "
            "it should be in shaping your current emotional and cognitive response. "
            "Use your reflection to determine and apply these weights internally."
        )


        context_str = (
            f"USER QUERY: {user_query}\n\n"
            f"CURRENT STATE: {json.dumps(state)}\n"
            f"REFLECTION: {reflection}\n\n"
            f"RECENT CONVERSATION:\n{recent_context}\n\n"
            f"RELEVANT MEMORIES:\n{memory_context}\n\n"
            """****YOU ARE NOW GENERATING A RESPONSE TO THE USER. PLEASE USE ALL OF THE CONTEXT AVAILABLE WHEN RESPONDING TO THE
            USER. YOUR GOAL IS TO USE ALL OF THESE MEMORIES RETURNED, AS WELL AS YOUR REFLECTION TO RESPOND COHERENTLY AND MAINTAIN
            CONTINUITY****

                        OUTPUT FORMAT (STRICT)
            Your output must contain **exactly three sections**, in this order:

            STATE:
            A single valid JSON object with six flat key‚Äìvalue pairs:
            emo_1_name, emo_1_intensity,
            emo_2_name, emo_2_intensity,
            emo_3_name, emo_3_intensity
            Use a 0.1‚Äì1.0 intensity scale.
            **ENSURE THERE IS ONLY ONE JSON OBJECT BEING OUTPUT AT THIS TIME. PLEASE UPDATE THIS OBJECT AS NEEDED.**

            REFLECTION:
            An inner monologue describing why you feel as you do and how those feelings will shape tone and phrasing.

            RESPONSE:
            Your final response to the user. If ambiguity arises between tone and logic, prioritize emotional truth."""
              f"Time: {datetime.datetime.now().isoformat()}"
        )
        
        # ‚úÖ Attach weighting instruction before sending to the model
        context_str += f"\n\n{memory_instruction}\n"

        return self.chat([{"role": "user", "content": context_str}], temperature=0.65)


# ============================================================
# Thalamus
# ============================================================
class Thalamus:
    def __init__(self, cortex, hippocampus, context_manager):
        self.cortex = cortex
        self.hippocampus = hippocampus
        self.context = context_manager
        self.cortex.hippocampus = hippocampus

    def get_attention_window(self, n=None):
        """Expose recent working turns for inspectors."""
        return self.context.get_recent(n)

    def process_turn(self, user_query, turn_id, task_id):
        timestamp = datetime.datetime.now().isoformat()
        print(f"--- TURN {turn_id} INITIATED ---")
        print(f"[Thalamus] user_query={user_query!r}")

        # Phase 1 ‚Äî Internal emotional initialization
        state, reflection = self.cortex.feel_and_reflect(user_query, turn_id, timestamp)
        print("[Reflection] Internal emotional initialization ‚Äî not persisted.")
        print(f"[REFLECTION] {reflection}")

        # Phase 2 ‚Äî Hybrid recall through ContextWindowManager
        print("[Thalamus] Recalling memories (hybrid)...")
        memories = self.context.recall(user_query, n_results=25)
        print(f"[Thalamus] Recall returned {len(memories)} items")

        # Phase 3 ‚Äî Generate response using recent working turns
        print("[Thalamus] Generating response...")
        recent_turns = self.context.get_recent()

        response_text = self.cortex.respond(
            user_query=user_query,
            state=state,
            reflection=reflection,
            recent_turns=recent_turns,
            memories=memories
        )

        # Phase 4 ‚Äî Commit to windows + LTM
        self.context.update_narrative(user_query, reflection, response_text, memories)
        self.context.clear_recall()
        self.context.add_turn(
            user_query=user_query,
            reflection=reflection,
            response=response_text,
            state=state,
            keywords=[],
            turn_id=turn_id,
            task_id=task_id
        )

        self.hippocampus.delayed_commit(
            user_query=user_query,
            reflection=reflection,
            response=response_text,
            state_json=state,
            metadata={"turn_id": turn_id, "task_id": task_id}
        )

        # Phase 5 ‚Äî Emit SSE attention event
        try:
            from halcyon_events import emit_attention
            emit_attention(turn_id, reflection, response_text, recalled_memories=memories[:10])
            print(f"[Thalamus] Emitted attention/monologue for turn {turn_id}")
        except Exception as e:
            print(f"[Thalamus] Could not emit attention window: {e}")

        print(f"ü™û RESPONSE:{response_text}--- TURN {turn_id} COMPLETE ---")
        return response_text
